{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5dEgRpy3952M"},"source":["## Load libraries\n","import numpy as np\n","import sympy as sp\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","plt.style.use('dark_background')\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Mount the Google Drive folder, if needed, for accessing data\n","if('google.colab' in sys.modules):\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount = True)\n","    # Change path below starting from /content/drive/MyDrive/Colab Notebooks/\n","    # depending on how data is organized inside your Colab Notebooks folder in\n","    # Google Drive\n","    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/OddSem2023MAHE'\n","    DATA_DIR = DIR+'/Data/'\n","else:\n","    DATA_DIR = 'Data/'"],"metadata":{"id":"69kMVA0KlRwX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QhXXZfGE9aLn"},"source":["---\n","\n","Calculating the softmax loss symbolically\n","\n","Consider the following 3 images with 4 features each:\n","\n","![](https://onedrive.live.com/embed?resid=37720F927B6DDC34%21101114&authkey=%21AEqz7RyEekVGRN8&width=1024)\n","\n","---"]},{"cell_type":"code","metadata":{"id":"CWW5t9U0zEjx"},"source":["# Define numpy arrays for each image\n","x1 = np.array([26, 100, 90, 80]) # Image-1\n","x2 = np.array([37, 58, 120, 96]) # Image-2\n","x3 = np.array([12, 30, 46, 54]) # Image-3\n","\n","# Build symbolic data matrix\n","X = sp.Matrix(np.stack((x1, x2, x3 ), axis = 1))\n","\n","# Pretty print symbolic matrix\n","sp.pprint(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtOzr96Z0P75"},"source":["# Create symbolic weight matrix\n","W = sp.MatrixSymbol('W', 3, 4)\n","\n","# Pretty print symbolic weight matrix\n","sp.pprint(sp.Matrix(W)) # Pretty print symbolic weight matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create symbolic bias vector\n","b = sp.MatrixSymbol('b', 3, 1)\n","\n","# Pretty print symbolic weight matrix\n","sp.pprint(sp.Matrix(b)) # Pretty print symbolic weight matrix"],"metadata":{"id":"_9veROwTvrT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kM918JKh02ic"},"source":["# Calculate symbolic raw scores\n","z0 = W*X[:, 0] + b\n","z1 = W*X[:, 1] + b\n","z2 = W*X[:, 2] + b\n","sp.pprint(z0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfjVmHXs1wF6"},"source":["# Calculate symbolic softmax loss for each image\n","loss_1 = -sp.log(sp.exp(z0[0,0])/(sp.exp(z0[0,0])+sp.exp(z0[1,0])+sp.exp(z0[2,0])))\n","loss_2 = -sp.log(sp.exp(z1[1,0])/(sp.exp(z1[0,0])+sp.exp(z1[1,0])+sp.exp(z1[2,0])))\n","loss_3 = -sp.log(sp.exp(z2[2,0])/(sp.exp(z2[0,0])+sp.exp(z2[1,0])+sp.exp(z2[2,0])))\n","\n","# Calculate symbolic average softmax loss\n","average_loss = (1/3)*(loss_1 + loss_2 + loss_3)\n","\n","# Pretty print symbolic average softmax loss\n","sp.pprint(average_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAK2713D7E3j"},"source":["# Evaluate average softmax loss for arbitrary weight and bias values\n","average_loss.evalf(subs = {W: sp.Matrix(np.random.randn(3,4)), b: sp.Matrix(np.random.randn(3,1))})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Function to visualize gradient descent in 1D\n","\n","---"],"metadata":{"id":"VZlZ62Bf3qka"}},{"cell_type":"code","source":["## Credit: https://github.com/pablocpz/Gradient-Descent-Visualizations/blob/main/1D%20simulation.ipynb\n","## Function for visualizing gradient descent in 1D\n","from matplotlib import animation\n","\n","def grad_1D(expr, w_values, w_initial, learning_rate, training_epochs, display_animation=False):\n","    \"\"\"\n","    expr : Sympy expression like (w+2)**2+3 (w symbol)\n","    w_values : np.linspace(a, b, n)\n","    w_initial: starting value\n","    learning_rate, training_epochs = z, r\n","    display_animation : if True, will return two objects to visualizing it (example below)\n","    \"\"\"\n","\n","    w = sp.symbols(\"w\")\n","    func = sp.lambdify(w, expr, \"numpy\")\n","    deriv = sp.diff(expr)\n","    deriv_func = sp.lambdify(w, deriv, \"numpy\")\n","\n","    #algorithm\n","    local_min = w_initial#np.random.choice(w_values,1)\n","    initial_local_min = local_min\n","    print(f\"Initial loss value {initial_local_min}\")\n","    model_params = np.zeros((training_epochs, 2)) #shape epochs x 2 cols\n","    for i in range(0, training_epochs):\n","        grad = deriv_func(local_min)\n","        local_min = local_min - (grad*learning_rate)\n","        model_params[i,0] = local_min\n","        model_params[i,1] = grad\n","    print(f\"Final loss value {training_epochs} epochs: {local_min}\")\n","\n","\n","    if display_animation:\n","        #prepare animation\n","        grad_fig, ax = plt.subplots(figsize=(6, 4), dpi=100)\n","\n","        ax.plot(w_values, func(w_values), label=f\"${sp.latex(expr)}$\")\n","        #ax.plot(w_values, deriv_func(w_values), label=f\"dL/dw ${sp.latex(deriv)}$\")\n","\n","        plt.title(f\"Empirical Local Minimum: {local_min}\")\n","\n","\n","        plt.axhline(0, color='white',linewidth=0.5)\n","        plt.axvline(0, color='white',linewidth=0.5)\n","\n","        plt.grid(color=\"gray\", linestyle=\"--\", linewidth=0.5)\n","        plt.xlabel(\"w\");\n","        plt.ylabel(\"L(w)\")\n","        #plt.legend();\n","        plt.close()\n","\n","        def tangent_line(x, x1, y1):\n","            # m*x+b\n","            return deriv_func(x1)*(x-x1) + y1\n","\n","        title = ax.set_title('', fontweight=\"bold\")\n","        local_min_scat = ax.scatter(initial_local_min, func(initial_local_min), color=\"orange\")\n","        initial_tangent_range = np.linspace(initial_local_min-0.5, initial_local_min+0.5, 10)\n","        tangent_plot = ax.plot(initial_tangent_range, tangent_line(x=initial_tangent_range,\n","                                                                   x1=initial_local_min,\n","                                                                   y1=func(initial_local_min)), linestyle=\"--\",  color=\"orange\", linewidth=2)[0]\n","        grad_annotation = ax.annotate(\n","            'Gradient={0:2f}'.format(deriv_func(initial_local_min)),\n","            xy=(initial_local_min,func(initial_local_min)), xytext=(initial_local_min,func(initial_local_min)+1),\n","            arrowprops = {'arrowstyle': \"-\", 'facecolor' : 'orange'},\n","            textcoords='data', color='orange' , rotation=20, fontweight=\"bold\"\n","        )\n","\n","        def drawframe(epoch):\n","            title.set_text('Epoch={0:4d}, learning rate = {1:2g}'.format(epoch,  learning_rate))\n","            x1 = model_params[epoch, 0]\n","            y1 = func(model_params[epoch, 0])\n","            local_min_scat.set_offsets((x1, y1))\n","            tangent_range = np.linspace(x1-0.5, x1+0.5, 10)\n","            tangent_values = tangent_line(x=tangent_range, x1=x1 ,y1=y1)\n","            tangent_plot.set_xdata(tangent_range)\n","            tangent_plot.set_ydata(tangent_values)\n","            grad_annotation.set_position((x1, y1+1))\n","            grad_annotation.xy = (x1, y1)\n","            grad_annotation.set_text('Gradient={0:2f}'.format(model_params[epoch, 1]))\n","            return local_min_scat,\n","\n","        # blit=True re-draws only the parts that have changed.\n","\n","        anim = animation.FuncAnimation(grad_fig, drawframe, frames=training_epochs, repeat=False, interval=500, blit=True)\n","\n","\n","        writer = animation.PillowWriter(fps=30,\n","                                        metadata=dict(artist='Me'),\n","                                        bitrate=1800)\n","        # ani.save('gradient1D.gif', writer=writer)\n","        # from IPython.display import HTML\n","        # HTML(anim.to_html5_video())\n","\n","        return anim, writer"],"metadata":{"id":"1dfcJerijDvz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Gradient descent visualization in 1D applied to $L(w) = (w+2)^2+3$ starting from $w=2.$\n","\n","---"],"metadata":{"id":"bun6kGrqeMNT"}},{"cell_type":"code","source":["## Generate animation for gradient descent in 1D\n","w = sp.symbols('w')\n","anim, writer = grad_1D(expr=(w+2)**2+3,\n","                       w_values=np.linspace(-6,6, 20),\n","                       w_initial = 2.0, learning_rate=0.1,\n","                       training_epochs=100,\n","                       display_animation=True)\n","#anim.save(DATA_DIR+'gradient1D.gif', writer=writer) #if you want to save it as GIF\n","from IPython.display import HTML\n","HTML(anim.to_html5_video()) #render the video"],"metadata":{"id":"NAGtLgBzkjD9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Gradient descent in 1D applied to $L(w) = (w+2)^2+3$ starting from $w=2.$\n","\n","----"],"metadata":{"id":"bdLvd9pz3keT"}},{"cell_type":"code","source":["## Gradient descent in 1D\n","L = lambda w: (w+2)**2 + 3\n","gradL = lambda w: 2*(w+2)\n","# Try 1e-05 (slow learning rate), 1e-01 (optimal),\n","# 1e0 (oscillates and does not converge),\n","# and 0.95 (oscillates towards the end and converges)\n","alpha = 0.95 # learning rate\n","tol = 1e-05 # stopping tolerance\n","iter = 0\n","maxiter = 1000\n","\n","w = 2 # starting point\n","\n","# Learning process\n","while np.abs(gradL(w)) > tol and iter < maxiter:\n","  w = w + alpha * -gradL(w)\n","  iter = iter+1\n","  print('Iteration = %d, w = %f, gradL(w) = %f'%(iter, w, gradL(w)))"],"metadata":{"id":"CgSWAKvAzbmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Gradient Descent in 2D applied to $L(\\mathbf{w}) = (w_1-2)^2+(w_2+3)^2$ starting from $w_1=1, w_2=1.$\n","\n","---"],"metadata":{"id":"EW4u318D4CdI"}},{"cell_type":"code","source":["# Gradient descent in 2D\n","L = lambda w: (w[0]-2)**2 + (w[1]+3)**2\n","gradL = lambda w: np.array([2*(w[0]-2), 2*(w[1]+3)])\n","alpha = 1e-02 # learning rate\n","tol = 1e-05 # stopping tolerance\n","iter = 0\n","maxiter = 1000\n","\n","w =  np.array([1, 1]) # initial guess\n","\n","while np.linalg.norm(gradL(w)) > tol and iter < maxiter:\n","  w = w + alpha *(-gradL(w))\n","  iter = iter+1\n","  print('Iteration = %d, w1 = %f, w2 = %f, ||gradL(w)|| = %f'%(iter, w[0], w[1], np.linalg.norm(gradL(w))))"],"metadata":{"id":"sSC_GF8n4E7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Calculating softmax loss and gradient for a toy dataset\n","\n","----"],"metadata":{"id":"KVeQ7C6KjBQ5"}},{"cell_type":"code","source":["# Generate artificial data with 5 samples, 4 features per sample\n","# and 3 output classes\n","num_samples = 5 # number of samples\n","num_features = 4 # number of features (a.k.a. dimensionality)\n","num_labels = 3 # number of output labels\n","# Data matrix (each column = single sample)\n","X = np.random.choice(np.arange(0, 5), size = (num_features, num_samples), replace = True)\n","# Class labels\n","y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n","# Randomly assign entries of weights matrix\n","W = np.random.choice(np.arange(-4, 4), size = (num_labels, num_features), replace = True)\n","print('X = ')\n","print(X)\n","print('y = ')\n","print(y)\n","print('W = ')\n","print(W)"],"metadata":{"id":"9xa6DTgxjFiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add the bias feature to the data matrix (run this cell only once!)\n","print('X = ')\n","print(X)\n","print('X with bias feature = ')\n","X = np.vstack([X, np.ones((1, num_samples))])\n","print(X)"],"metadata":{"id":"Rzl6d41LjIlq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust the weight matrix with (possibly random) values added\n","# for bias as the last column (run this cell only once!)\n","W = np.hstack([W, np.ones((num_labels, 1))])\n","print(W)"],"metadata":{"id":"HgBslwcBjMwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the raw zcores matrix\n","Z =  np.dot(W, X)\n","print('Z = ')\n","print(Z)"],"metadata":{"id":"G-vIsPOMjPqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert scores to non-normalized probabilites matrix. Note that for each sample,\n","# that is in each column, the values don't add up to 1. Also note that the\n","# output values are typically large or small\n","P = np.exp(Z)\n","print('P = ')\n","P"],"metadata":{"id":"ZCmnD8dvjQ8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Normalize probabilities matrix such that the sum across each column is equal to 1.\n","# Now we have actually probability values for each sample.\n","P = P / np.sum(P, axis = 0)\n","print('P = ')\n","print(P)\n","# Sum in each column of matrix P\n","print(np.sum(P, axis = 0))\n","# Print the correct label for each sample\n","print(y)"],"metadata":{"id":"qkK2FJk2jVjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate loss for all samples.\n","loss =  -np.log(P[y, np.arange(num_samples)])\n","print('Loss = ')\n","print(loss)\n","# Calculate average training loss\n","loss_data = np.mean(loss)\n","print('Total loss = %f'%(loss_data))"],"metadata":{"id":"Dn31dhCUji5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust the probability matrix such that 1 is subtracted\n","# from each samples correct category probability\n","P[y, range(num_samples)] = P[y, range(num_samples)] - 1"],"metadata":{"id":"qm3WmbYpjw-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the gradient of total loss w.r.t. the weights W\n","dW = (1/num_samples)*np.dot(P, X.T)\n","print(dW)"],"metadata":{"id":"HD7bPf17jy8H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Applying $L_2$-regularization (loss and gradient)\n","\n","---"],"metadata":{"id":"2fxhDs4S-XVj"}},{"cell_type":"code","source":["# Generate artificial data with 5 samples, 4 features per sample\n","# and 3 output classes\n","num_samples = 5 # number of samples\n","num_features = 4 # number of features (a.k.a. dimensionality)\n","num_labels = 3 # number of output labels\n","# Data matrix (each column = single sample)\n","X = np.random.choice(np.arange(0, 5), size = (num_features, num_samples), replace = True)\n","# Class labels\n","y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n","# Randomly assign entries of weights matrix\n","W = np.random.choice(np.arange(-4, 4), size = (num_labels, num_features), replace = True)\n","# Add the bias feature to the data matrix\n","X = np.vstack([X, np.ones((1, num_samples))])\n","# Adjust the weight matrix with (possibly random) values added\n","# for bias as the last column\n","W = np.hstack([W, np.ones((num_labels, 1))])\n","# Calculate the raw zcores matrix\n","Z =  np.dot(W, X)\n","# Convert scores to non-normalized probabilites matrix. Note that for each sample,\n","# that is in each column, the values don't add up to 1. Also note that the\n","# output values are typically large or small\n","P = np.exp(Z)\n","# Normalize probabilities matrix such that the sum across each column is equal to 1.\n","# Now we have actually probability values for each sample.\n","P = P / np.sum(P, axis = 0)"],"metadata":{"id":"X-V1KQio-eZW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate loss for all samples.\n","loss =  -np.log(P[y, np.arange(num_samples)])\n","print('Loss = ')\n","print(loss)\n","# Calculate total average data loss\n","loss_data = np.mean(loss)\n","# Regularization loss\n","alpha = 0.1 # strength of regularization = 10%\n","loss_reg = np.sum(W[:, :-1] * W[:, :-1])\n","print('Total loss = %f'%(loss_data + alpha*loss_reg))"],"metadata":{"id":"56BxHGw2-2MN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjust the probability matrix such that 1 is subtracted\n","# from each samples correct category probability\n","P[y, range(num_samples)] = P[y, range(num_samples)] - 1"],"metadata":{"id":"RQsRN3p__Tp5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the gradient of total loss w.r.t. the weights W\n","dW = (1/num_samples)*np.dot(P, X.T) +2 * alpha* np.hstack([W[:, :-1], np.zeros((num_labels, 1))])\n","print(dW)"],"metadata":{"id":"AB8gmxRq_WPy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Batch Processing\n","\n","---"],"metadata":{"id":"tiYEh21B_ffp"}},{"cell_type":"code","source":["# Demonstration of splitting samples into batches for batch processing\n","# using a simple example\n","\n","num_samples = 11 # total number of samples\n","num_iters = 10   # number of iterations\n","batch_size = 3   # number of samples for calculating loss and gradient in each iteration\n","\n","print('Number of samples = %d'%(num_samples))\n","print('Number of iterations = %d'%(num_iters))\n","print('Batch size = %d'%(batch_size))\n","\n","# Function to generate sample indices for batch processing according to batch size\n","def generate_batch_indices():\n","  # Reorder sample indices\n","  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n","  # Generate batch indices for batch processing\n","  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n","  return(batch_indices)\n","\n","# Number of batches per epoch\n","num_iterations_per_epoch = int(np.ceil(num_samples/batch_size))\n","print('Number of iterations per epoch = %d\\n'%(num_iterations_per_epoch))\n","b = 0\n","epoch = 0\n","for it in range(num_iters):\n","  if it % num_iterations_per_epoch == 0:# check if we are at the start of an epoch\n","    print('--------------------------------')\n","    print('Epoch %d:'%(epoch+1))\n","    batch_indices = generate_batch_indices()\n","    b = 0\n","    epoch = epoch + 1\n","    print('--------------------------------')\n","  print('In iteration %d, using samples' % (it+1))\n","  print(batch_indices[b])\n","  b += 1"],"metadata":{"id":"cMge-PPO_sJw"},"execution_count":null,"outputs":[]}]}